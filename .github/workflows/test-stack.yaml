name: Test Stack
on: 
    workflow_call:
      inputs:
        namespace:
          description: 'namespace to test'
          type: string
          required: true
        target_branch:
          description: 'branch to test'
          type: string
          required: false
          default: main
        comparison:
          description: 'run comparison tests'
          type: boolean
          required: false
          default: false
        comparison_namespace:
          description: 'namespace to run comparison tests in'
          type: string
          required: false
          default: ""
      outputs:
        passed:
           description: "true if tests passed, false otherwise"
           value: ${{ jobs.test-stack.outputs.testsFailed == 'false' }}
jobs:
    test-stack:
      runs-on: ubuntu-latest
      outputs:
        testsFailed: ${{ steps.verify.outputs.didfail }}
        failed: ${{ steps.verify.outputs.didfail }}
      steps:
        - name: Checkout repo
          uses: actions/checkout@v4
          with:
            repository: 'opencost/opencost-integration-tests'
            fetch-depth: 0
            submodules: recursive
        - name: checkout target branch
          run: |
            git checkout ${{ inputs.target_branch }} || true
        
        - id: run-tests
          name: run tests
          env:
            BATS_TEST_RETRIES: 2
            COMPARISON: ${{ inputs.comparison }}
            COMPARISON_NAMESPACE: ${{ inputs.comparison_namespace }}
            NAMESPACE: ${{ inputs.namespace }}
          run: |

               export OPENCOST_URL="http://$NAMESPACE.infra.opencost.io/model"
               export COMPARISON_OPENCOST_URL="http://$COMPARISON_NAMESPACE.infra.opencost.io/model"
               export OPENCOST_MCP_URL="https://$NAMESPACE-mcp.infra.opencost.io"
               # Wait for OpenCost API to be ready before running tests
               echo "Waiting for OpenCost API to be available at $OPENCOST_URL..."
               
               max_attempts=8
               attempt=1
               wait_seconds=10
               
               while [ $attempt -le $max_attempts ]; do
                 echo "Attempt $attempt/$max_attempts: Checking if OpenCost API is ready..."
                 
                 http_code=$(curl -L -s -o /dev/null -w "%{http_code}" $OPENCOST_URL/assets?window=10m)
                 
                 if [ "$http_code" = "200" ]; then
                   echo "OpenCost API is ready! Received HTTP 200 response."
                   break
                 else
                   echo "OpenCost API returned HTTP code $http_code, not ready yet. Waiting $wait_seconds seconds..."
                   sleep $wait_seconds
                   attempt=$((attempt + 1))
                 fi
                 
                 if [ $attempt -gt $max_attempts ]; then
                   echo "Error: OpenCost API did not become available after $(($max_attempts * $wait_seconds)) seconds."
                   exit 1
                 fi
               done
               
               echo "Proceeding with tests..."
               if [ "$COMPARISON" = "true" ]; then
                echo "Running comparison tests..."
                ./test/bats/bin/bats -T --no-parallelize-within-files --jobs 4 -r test/comparison | tee results.txt
               else
                echo "Running integration tests..."
                ./test/bats/bin/bats -T --no-parallelize-within-files --jobs 4 -r test/integration | tee results.txt
               fi
        - id: verify
          name: verify test results
          run: |
            cat results.txt
            if grep -q "FAIL\|not ok" results.txt; then
              echo "didfail=true"
              echo "didfail=true" >> "$GITHUB_OUTPUT"
            else
              echo "didfail=false"
              echo "didfail=false" >> "$GITHUB_OUTPUT"
            fi
        - name: upload test results
          uses: actions/upload-artifact@v4
          with:
            retention-days: 1
            name: test-results-${{ github.run_number }}
            path: results.txt
            overwrite: true
        - name: Fail if tests failed
          if: always() &&  ( failure() || steps.verify.outputs.didfail == 'true')
          run: exit 1  
